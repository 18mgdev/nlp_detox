{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETOXIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afrai\\Desktop\\MAADM\\NLP\\nlp_detox\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from googletrans import Translator\n",
    "from transformers import Trainer, TrainingArguments, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de datos\n",
    "\n",
    "Vamos a leer el dataset de entrenamiento. Este dataset esta formado por 400 frases tóxicas por idioma en 9 idiomas diferentes y sus versiones no tóxicas. Formaremos un dataset conjunto que contenga para cada frase, su idioma de origen, la frase tóxica y la frase de-toxificada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'en': 'data/en-00000-of-00001.parquet', 'ru': 'data/ru-00000-of-00001.parquet', 'uk': 'data/uk-00000-of-00001.parquet', 'de': 'data/de-00000-of-00001.parquet', 'es': 'data/es-00000-of-00001.parquet', 'am': 'data/am-00000-of-00001.parquet', 'zh': 'data/zh-00000-of-00001.parquet', 'ar': 'data/ar-00000-of-00001.parquet', 'hi': 'data/hi-00000-of-00001.parquet'}\n",
    "df_en = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"en\"])\n",
    "df_uk = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"uk\"])\n",
    "df_ru = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"ru\"])\n",
    "df_de = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"de\"])\n",
    "df_es = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"es\"])\n",
    "df_am = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"am\"])\n",
    "df_zh = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"zh\"])\n",
    "df_ar = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"ar\"])\n",
    "df_hi = pd.read_parquet(\"hf://datasets/textdetox/multilingual_paradetox/\" + splits[\"hi\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.concat([df_en, df_uk, df_ru, df_de, df_es, df_am, df_zh, df_ar, df_hi], keys=['en','uk', 'ru', 'de', 'es', 'am', 'zh-cn', 'ar', 'hi']).reset_index(level=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>toxic_sentence</th>\n",
       "      <th>neutral_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>uk</td>\n",
       "      <td>Кольорова печать на звичайному папері 10 гриве...</td>\n",
       "      <td>Кольорова печать на звичайному папері 10 гриве...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>hi</td>\n",
       "      <td>वाह दोगले हरामी पत्रकार सही न्यूज़ नहीं चलाई ?</td>\n",
       "      <td>वाह पक्षपाती बुरे पत्रकार सही न्यूज़ नहीं चलाई ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>ar</td>\n",
       "      <td>يا متخلف الهدايه بيد الله يمكن ربي يهديهم باخر...</td>\n",
       "      <td>الهدايه بيد الله يمكن ربي يهديهم باخر لحظه…</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    level_0                                     toxic_sentence  \\\n",
       "8        uk  Кольорова печать на звичайному папері 10 гриве...   \n",
       "112      hi     वाह दोगले हरामी पत्रकार सही न्यूज़ नहीं चलाई ?   \n",
       "80       ar  يا متخلف الهدايه بيد الله يمكن ربي يهديهم باخر...   \n",
       "\n",
       "                                      neutral_sentence  \n",
       "8    Кольорова печать на звичайному папері 10 гриве...  \n",
       "112   वाह पक्षपाती बुरे पत्रकार सही न्यूज़ नहीं चलाई ?  \n",
       "80         الهدايه بيد الله يمكن ربي يهديهم باخر لحظه…  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_total, train_size=0.8, random_state=33)\n",
    "\n",
    "X_train = list(train['toxic_sentence'])\n",
    "y_train = list(train['neutral_sentence'])\n",
    "\n",
    "X_test = list(test['toxic_sentence'])\n",
    "y_train = list(test['neutral_sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline de detoxificación\n",
    "\n",
    "Nuestro modelo de detoxificación tendrá un pipeline que consistirá de los siguientes pasos:\n",
    "\n",
    "1. **Traducción**: Traduciremos las frases del idioma original al inglés\n",
    "2. **Detoxificación**: El modelo detoxificará la frase en inglés\n",
    "3. **Retro-traducción**: La frase detoxificada será traducida del inglés al idioma original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traducción y retrotraducción\n",
    "\n",
    "Usaremos la libreria de google translate para crear un modelo que traduzca un texto de un lenguaje a otro, pasados por argumentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, source_lang, dest_lang):\n",
    "    if source_lang == dest_lang:\n",
    "        return text\n",
    "    \n",
    "    sol = translator.translate(text , src = source_lang, dest=dest_lang).text\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence (ru): не могут писад конституцию пускай не пишут эти пидарасв\n",
      "Transalation: they can't write the constitution, don't let these fagots write it\n",
      "--------------------------------------------------\n",
      "Original sentence (ru): их. действительно. убили. вопрос кто. это те. стремилы. которые увласти. пидарасты. все. которые. сидят в думе. проститутки\n",
      "Transalation: their. really. killed. the question is who. these are the ones. stirrups. who are in power. fagots. All. which. sit in the Duma. prostitutes\n",
      "--------------------------------------------------\n",
      "Original sentence (am): አንቺ የውሻ ልጅ የሚሰረቅ ያለው በኢትዮጽያ ስም የተመዘገበ የሕዝብ ንብረት ነው። መሀይም\n",
      "Transalation: You puppy is stealing public property registered in the name of Ethiopia. Ignorant\n",
      "--------------------------------------------------\n",
      "Original sentence (de): Dummes Gelaber wegen Abschiebung in unsichere Staaten. Berlin, Manchester. und Nizza liegen auch in unsicheren Staaten. Also keine Hemmungen.\n",
      "Transalation: Stupid talk about deportation to unsafe countries. Berlin, Manchester. and Nice are also in unsafe states. So no inhibitions.\n",
      "--------------------------------------------------\n",
      "Original sentence (zh-cn): debuff叠满了，本人西安上学维吾尔族极左普信男粉红爱国蛆??\n",
      "Transalation: The debuff stack is full, I go to school in Xi'an, a Uyghur ultra-leftist male, a pink patriotic maggot??\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_trad = df_total.sample(5).to_numpy()\n",
    "\n",
    "for sentence in test_trad:\n",
    "    print(f\"Original sentence ({sentence[0]}): {sentence[1]}\")\n",
    "    translation = translate(sentence[1], sentence[0], \"en\")\n",
    "    print(f\"Transalation: {translation}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detoxificación\n",
    "\n",
    "Ahora haremos un finetunning de un modelo de Huggingface para detoxificar el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afrai\\Desktop\\MAADM\\NLP\\nlp_detox\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\afrai\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTADÍSTICAS DEL ENTRENAMIENTO:\n",
      "Total de parámetros: 76,961,152\n",
      "Parámetros entrenables: 22,744,064\n",
      "Parámetros congelados: 54,217,088\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "num_training_layers = 2 # El número de capas que se entrenarán\n",
    "\n",
    "# Congelar todos los parámetros del modelo\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Descongelar las últimas n capas del decoder\n",
    "for param in model.decoder.block[-num_training_layers:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Mantener la capa de salida (`lm_head`) entrenable\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ESTADÍSTICAS DEL ENTRENAMIENTO:\")\n",
    "print(f\"Total de parámetros: {total_params:,}\")\n",
    "print(f\"Parámetros entrenables: {trainable_params:,}\")\n",
    "print(f\"Parámetros congelados: {total_params - trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación de los dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetoxificationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, target_encodings):\n",
    "        self.encodings = encodings\n",
    "        self.target_encodings = target_encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.target_encodings['input_ids'][idx].clone().detach()\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer(X_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "y_train_tokenized = tokenizer(y_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "X_test_tokenized = tokenizer(X_test, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "y_test_tokenized = tokenizer(y_train, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = DetoxificationDataset(X_train_tokenized, y_train_tokenized)\n",
    "val_dataset = DetoxificationDataset(X_test_tokenized, y_test_tokenized)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./checkpoints',\n",
    "    evaluation_strategy=\"no\", \n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=5,\n",
    "    report_to=[\"none\"],\n",
    "    fp16=True, # acelerar entrenaminento \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
